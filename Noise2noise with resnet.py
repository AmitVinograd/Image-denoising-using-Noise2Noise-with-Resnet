# -*- coding: utf-8 -*-
"""Final_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14ULNi2VGXKAFtThXt9kvC-rBwqM7JOWu

## Unet
"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D, concatenate
from tensorflow.keras.models import Model

# Suppress warnings
import warnings
warnings.filterwarnings('ignore')

def add_noise(images, noise_type='gaussian', params=None):
    """
    Add specified type of noise to images.
    """
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'gaussian':
        noise_factor = params.get('factor', 0.3)
        gaussian_noise = noise_factor * np.random.normal(
            loc=0.0, scale=1.0, size=images.shape)
        noisy_images = images + gaussian_noise

    elif noise_type == 'poisson':
        scaling_factor = params.get('scaling_factor', 30.0)
        scaled_images = images * scaling_factor
        noisy_images = np.random.poisson(scaled_images) / scaling_factor

    elif noise_type == 'bernoulli':
        prob = params.get('prob', 0.05)
        mask = np.random.binomial(n=1, p=prob, size=images.shape)
        salt = np.random.binomial(n=1, p=0.5, size=images.shape)
        noisy_images = np.where(mask == 1, salt, images)

    return np.clip(noisy_images, 0., 1.)

def preprocess(image, label):
    """
    Preprocess images.
    """
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image

def prepare_data():
    """
    Prepare dataset and return all images without filtering.
    """
    # Load Caltech101 dataset
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    # Preprocess datasets
    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    # Convert to numpy arrays
    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test


    # Prepare datasets
    BATCH_SIZE = 32
    train_dataset = train_dataset.filter(filter_cars).map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.filter(filter_cars).map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    # Convert to numpy arrays
    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test

def create_unet_model(input_shape=(224, 224, 3)):
    """
    Create U-Net model for Noise2Noise training
    """
    inputs = Input(input_shape)

    # Encoder
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)
    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)
    pool1 = MaxPooling2D((2, 2))(conv1)

    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)
    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)
    pool2 = MaxPooling2D((2, 2))(conv2)

    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)
    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)
    pool3 = MaxPooling2D((2, 2))(conv3)

    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)
    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)

    # Decoder
    up5 = UpSampling2D((2, 2))(conv4)
    up5 = Conv2D(128, (2, 2), activation='relu', padding='same')(up5)
    merge5 = concatenate([conv3, up5], axis=3)
    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(merge5)
    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)

    up6 = UpSampling2D((2, 2))(conv5)
    up6 = Conv2D(64, (2, 2), activation='relu', padding='same')(up6)
    merge6 = concatenate([conv2, up6], axis=3)
    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(merge6)
    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)

    up7 = UpSampling2D((2, 2))(conv6)
    up7 = Conv2D(32, (2, 2), activation='relu', padding='same')(up7)
    merge7 = concatenate([conv1, up7], axis=3)
    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(merge7)
    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)

    output = Conv2D(3, (1, 1), activation='sigmoid')(conv7)

    model = Model(inputs=[inputs], outputs=[output])
    model.compile(optimizer='adam', loss='mse', metrics=['mae'])

    return model

def train_noise2noise(epochs=50):
    """
    Train and evaluate models using Noise2Noise approach
    """
    # Prepare data
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    # Define noise configurations
    noise_configs = {
        'gaussian': {'factor': 0.3},
        'poisson': {'scaling_factor': 30.0},
        'bernoulli': {'prob': 0.05}
    }

    # Store results
    models = {}
    histories = {}
    noisy_images = {}
    denoised_images = {}
    metrics = {}

    # Train a model for each noise type
    for noise_type, params in noise_configs.items():
        print(f"\nTraining Noise2Noise model for {noise_type} noise...")

        # Create two different noisy versions for training
        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=params)

        # Create noisy test images
        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=params)
        noisy_images[noise_type] = X_test_noisy

        # Create and train model (Noise2Noise approach: train on noisy pairs)
        model = create_unet_model()
        history = model.fit(X_train_noisy1, X_train_noisy2,  # Train on noisy-to-noisy pairs
                          epochs=epochs,
                          batch_size=32,
                          shuffle=True,
                          validation_split=0.1,
                          verbose=1)

        # Store results
        models[noise_type] = model
        histories[noise_type] = history
        denoised_images[noise_type] = model.predict(X_test_noisy)
        metrics[noise_type] = model.evaluate(X_test_noisy, X_test)  # Still evaluate against clean images

        # Plot training history
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    # Display comparison results
    n_samples = min(3, len(X_test))
    n_rows = len(noise_configs) * 2 + 1
    fig, axes = plt.subplots(n_rows, n_samples, figsize=(15, 4*n_rows))

    # Original images
    for i in range(n_samples):
        axes[0, i].imshow(X_test[i])
        axes[0, i].axis('off')
        if i == 0:
            axes[0, i].set_title('Original')

    # Results for each noise type
    current_row = 1
    for noise_type in noise_configs.keys():
        for i in range(n_samples):
            # Noisy image
            axes[current_row, i].imshow(noisy_images[noise_type][i])
            axes[current_row, i].axis('off')
            if i == 0:
                axes[current_row, i].set_title(f'Noisy ({noise_type})')

            # Denoised image
            axes[current_row + 1, i].imshow(denoised_images[noise_type][i])
            axes[current_row + 1, i].axis('off')
            if i == 0:
                axes[current_row + 1, i].set_title(f'Denoised ({noise_type})')

        current_row += 2

    plt.tight_layout()
    plt.show()

    # Print metrics
    print("\nTest Metrics:")
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")

# Run the training and evaluation
if __name__ == "__main__":
    train_noise2noise(epochs=50)

"""# resnet50"""

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.applications import ResNet50, VGG16
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')


def add_noise(images, noise_type='gaussian', params=None):
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'gaussian':
        noise_factor = params.get('factor', 0.3)
        gaussian_noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
        noisy_images = images + gaussian_noise

    elif noise_type == 'poisson':
        scaling_factor = params.get('scaling_factor', 30.0)
        scaled_images = images * scaling_factor
        noisy_images = np.random.poisson(scaled_images) / scaling_factor

    elif noise_type == 'bernoulli':
        prob = params.get('prob', 0.05)
        mask = np.random.binomial(n=1, p=prob, size=images.shape)
        salt = np.random.binomial(n=1, p=0.5, size=images.shape)
        noisy_images = np.where(mask == 1, salt, images)

    return np.clip(noisy_images, 0., 1.)


def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image


def prepare_data():
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test


def perceptual_loss(y_true, y_pred):
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
    true_features = model(y_true)
    pred_features = model(y_pred)
    return tf.reduce_mean(tf.square(true_features - pred_features))


def create_resnet50_with_skips(input_shape=(224, 224, 3)):
    base_model = ResNet50(include_top=False, weights='imagenet', input_shape=input_shape)

    # Encoder features
    skip1 = base_model.get_layer("conv1_relu").output
    skip2 = base_model.get_layer("conv2_block3_out").output
    skip3 = base_model.get_layer("conv3_block4_out").output
    skip4 = base_model.get_layer("conv4_block6_out").output
    base_output = base_model.output

    # Decoder
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(base_output)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip4])
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip3])
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip2])
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip1])
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='mse', metrics=['mae'])
    return model


def visualize_results(original_images, noisy_images, denoised_images, num_images=3, noise_type=""):
    plt.figure(figsize=(15, num_images * 5))

    for i in range(num_images):
        plt.subplot(num_images, 3, i * 3 + 1)
        plt.imshow(original_images[i])
        plt.title("Original")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 2)
        plt.imshow(noisy_images[i])
        plt.title(f"Noisy ({noise_type})")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 3)
        plt.imshow(np.clip(denoised_images[i], 0, 1))
        plt.title("Denoised")
        plt.axis("off")

    plt.tight_layout()
    plt.show()


def train_noise2noise(epochs=50):
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    noise_configs = {
        'gaussian': {'factor': 0.3},
        'poisson': {'scaling_factor': 30.0},
        'bernoulli': {'prob': 0.05}
    }

    models = {}
    histories = {}
    noisy_images = {}
    denoised_images = {}
    metrics = {}

    for noise_type, params in noise_configs.items():
        print(f"\nTraining Noise2Noise model for {noise_type} noise...")

        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=params)

        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=params)
        noisy_images[noise_type] = X_test_noisy

        model = create_resnet50_with_skips()
        history = model.fit(X_train_noisy1, X_train_noisy2,
                            epochs=epochs,
                            batch_size=32,
                            shuffle=True,
                            validation_split=0.1,
                            verbose=1)

        models[noise_type] = model
        histories[noise_type] = history
        denoised_images[noise_type] = model.predict(X_test_noisy)
        metrics[noise_type] = model.evaluate(X_test_noisy, X_test)

        visualize_results(
            X_test[:3],
            X_test_noisy[:3],
            denoised_images[noise_type][:3],
            noise_type=noise_type
        )

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    print("\nTest Metrics:")
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")


if __name__ == "__main__":
    train_noise2noise(epochs=50)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.applications import ResNet101, VGG16
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

def add_noise(images, noise_type='gaussian', params=None):
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'gaussian':
        noise_factor = params.get('factor', 0.3)
        gaussian_noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
        noisy_images = images + gaussian_noise

    elif noise_type == 'poisson':
        scaling_factor = params.get('scaling_factor', 30.0)
        scaled_images = images * scaling_factor
        noisy_images = np.random.poisson(scaled_images) / scaling_factor

    elif noise_type == 'bernoulli':
        prob = params.get('prob', 0.05)
        mask = np.random.binomial(n=1, p=prob, size=images.shape)
        salt = np.random.binomial(n=1, p=0.5, size=images.shape)
        noisy_images = np.where(mask == 1, salt, images)

    return np.clip(noisy_images, 0., 1.)

def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image

def prepare_data():
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test

def perceptual_loss(y_true, y_pred):
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
    true_features = model(y_true)
    pred_features = model(y_pred)
    return tf.reduce_mean(tf.square(true_features - pred_features))

def create_resnet101_with_skips(input_shape=(224, 224, 3)):
    base_model = ResNet101(include_top=False, weights='imagenet', input_shape=input_shape)

    # Encoder features
    skip1 = base_model.get_layer("conv1_relu").output
    skip2 = base_model.get_layer("conv2_block3_out").output
    skip3 = base_model.get_layer("conv3_block4_out").output
    skip4 = base_model.get_layer("conv4_block23_out").output
    base_output = base_model.output

    # Decoder
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(base_output)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip4])
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip3])
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip2])
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip1])
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='mse', metrics=['mae'])
    return model

def visualize_results(original_images, noisy_images, denoised_images, num_images=3, noise_type=""):
    plt.figure(figsize=(15, num_images * 5))

    for i in range(num_images):
        plt.subplot(num_images, 3, i * 3 + 1)
        plt.imshow(original_images[i])
        plt.title("Original")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 2)
        plt.imshow(noisy_images[i])
        plt.title(f"Noisy ({noise_type})")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 3)
        plt.imshow(np.clip(denoised_images[i], 0, 1))
        plt.title("Denoised")
        plt.axis("off")

    plt.tight_layout()
    plt.show()

def train_noise2noise(epochs=50):
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    # Remove Bernoulli noise from noise configurations
    noise_configs = {
        'gaussian': {'factor': 0.3},
        'poisson': {'scaling_factor': 30.0}
    }

    models = {}
    histories = {}
    noisy_images = {}
    denoised_images = {}
    metrics = {}

    for noise_type, params in noise_configs.items():
        print(f"\nTraining Noise2Noise model for {noise_type} noise...")

        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=params)

        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=params)
        noisy_images[noise_type] = X_test_noisy

        model = create_resnet101_with_skips()
        history = model.fit(X_train_noisy1, X_train_noisy2,
                            epochs=epochs,
                            batch_size=32,
                            shuffle=True,
                            validation_split=0.1,
                            verbose=1)

        models[noise_type] = model
        histories[noise_type] = history
        denoised_images[noise_type] = model.predict(X_test_noisy)
        metrics[noise_type] = model.evaluate(X_test_noisy, X_test)

        visualize_results(
            X_test[:3],
            X_test_noisy[:3],
            denoised_images[noise_type][:3],
            noise_type=noise_type
        )

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    print("\nTest Metrics:")
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")

if __name__ == "__main__":
    train_noise2noise(epochs=50)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.applications import ResNet101, VGG16
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')

def add_noise(images, noise_type='gaussian', params=None):
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'gaussian':
        noise_factor = params.get('factor', 0.3)
        gaussian_noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
        noisy_images = images + gaussian_noise

    elif noise_type == 'poisson':
        scaling_factor = params.get('scaling_factor', 30.0)
        scaled_images = images * scaling_factor
        noisy_images = np.random.poisson(scaled_images) / scaling_factor

    elif noise_type == 'bernoulli':
        prob = params.get('prob', 0.05)
        mask = np.random.binomial(n=1, p=prob, size=images.shape)
        salt = np.random.binomial(n=1, p=0.5, size=images.shape)
        noisy_images = np.where(mask == 1, salt, images)

    return np.clip(noisy_images, 0., 1.)

def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image

def prepare_data():
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test

def perceptual_loss(y_true, y_pred):
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
    true_features = model(y_true)
    pred_features = model(y_pred)
    return tf.reduce_mean(tf.square(true_features - pred_features))

def create_resnet101_with_skips(input_shape=(224, 224, 3)):
    base_model = ResNet101(include_top=False, weights='imagenet', input_shape=input_shape)

    # Encoder features
    skip1 = base_model.get_layer("conv1_relu").output
    skip2 = base_model.get_layer("conv2_block3_out").output
    skip3 = base_model.get_layer("conv3_block4_out").output
    skip4 = base_model.get_layer("conv4_block23_out").output
    base_output = base_model.output

    # Decoder
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(base_output)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip4])
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip3])
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip2])
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)
    x = concatenate([x, skip1])
    x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)
    x = UpSampling2D((2, 2))(x)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='mse', metrics=['mae'])
    return model

def visualize_results(original_images, noisy_images, denoised_images, num_images=3, noise_type=""):
    plt.figure(figsize=(15, num_images * 5))

    for i in range(num_images):
        plt.subplot(num_images, 3, i * 3 + 1)
        plt.imshow(original_images[i])
        plt.title("Original")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 2)
        plt.imshow(noisy_images[i])
        plt.title(f"Noisy ({noise_type})")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 3)
        plt.imshow(np.clip(denoised_images[i], 0, 1))
        plt.title("Denoised")
        plt.axis("off")

    plt.tight_layout()
    plt.show()

def train_noise2noise(epochs=50):
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    # Only use Bernoulli noise
    noise_configs = {
        'bernoulli': {'prob': 0.05}  # Bernoulli noise with a probability of 0.05
    }

    models = {}
    histories = {}
    noisy_images = {}
    denoised_images = {}
    metrics = {}

    for noise_type, params in noise_configs.items():
        print(f"\nTraining Noise2Noise model for {noise_type} noise...")

        # Generate noisy images
        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=params)

        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=params)
        noisy_images[noise_type] = X_test_noisy

        # Create model and reduce learning rate
        model = create_resnet101_with_skips()
        history = model.fit(X_train_noisy1, X_train_noisy2,
                            epochs=epochs,
                            batch_size=16,  # Reduced batch size for memory efficiency
                            shuffle=True,
                            validation_split=0.1,
                            verbose=1,
                            callbacks=[tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)])  # Early stopping

        models[noise_type] = model
        histories[noise_type] = history
        denoised_images[noise_type] = model.predict(X_test_noisy)
        metrics[noise_type] = model.evaluate(X_test_noisy, X_test)

        visualize_results(
            X_test[:3],
            X_test_noisy[:3],
            denoised_images[noise_type][:3],
            noise_type=noise_type
        )

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    print("\nTest Metrics:")
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")

if __name__ == "__main__":
    train_noise2noise(epochs=50)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.applications import ResNet152, VGG16
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import warnings

# Suppress warnings
warnings.filterwarnings('ignore')


def add_noise(images, noise_type='gaussian', params=None):
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'gaussian':
        noise_factor = params.get('factor', 0.3)
        gaussian_noise = noise_factor * np.random.normal(loc=0.0, scale=1.0, size=images.shape)
        noisy_images = images + gaussian_noise

    elif noise_type == 'poisson':
        scaling_factor = params.get('scaling_factor', 30.0)
        scaled_images = images * scaling_factor
        noisy_images = np.random.poisson(scaled_images) / scaling_factor

    return np.clip(noisy_images, 0., 1.)


def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image


def prepare_data():
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test


def perceptual_loss(y_true, y_pred):
    vgg = VGG16(include_top=False, weights='imagenet', input_shape=(224, 224, 3))
    model = Model(inputs=vgg.input, outputs=vgg.get_layer('block3_conv3').output)
    true_features = model(y_true)
    pred_features = model(y_pred)
    return tf.reduce_mean(tf.square(true_features - pred_features))


def create_resnet152_with_skips(input_shape=(224, 224, 3)):
    base_model = ResNet152(include_top=False, weights='imagenet', input_shape=input_shape)

    # Encoder features
    skip1 = base_model.get_layer("conv1_conv").output        # 112x112
    skip2 = base_model.get_layer("conv2_block3_1_relu").output  # 56x56
    skip3 = base_model.get_layer("conv3_block8_1_relu").output  # 28x28
    skip4 = base_model.get_layer("conv4_block36_1_relu").output # 14x14
    skip5 = base_model.get_layer("conv5_block3_1_relu").output  # 7x7
    base_output = base_model.output                          # 7x7

    # Decoder with feature pyramid structure
    x = Conv2D(2048, (3, 3), activation='relu', padding='same')(base_output)
    x = concatenate([x, skip5])
    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip4])
    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip3])
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip2])
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip1])
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(
        optimizer=Adam(learning_rate=1e-5),
        loss='mse',
        metrics=['mae']
    )

    return model


def visualize_results(original_images, noisy_images, denoised_images, num_images=3, noise_type=""):
    plt.figure(figsize=(15, num_images * 5))

    for i in range(num_images):
        plt.subplot(num_images, 3, i * 3 + 1)
        plt.imshow(original_images[i])
        plt.title("Original")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 2)
        plt.imshow(noisy_images[i])
        plt.title(f"Noisy ({noise_type})")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 3)
        plt.imshow(np.clip(denoised_images[i], 0, 1))
        plt.title("Denoised")
        plt.axis("off")

    plt.tight_layout()
    plt.show()


def train_noise2noise(epochs=50):
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    noise_configs = {
        'gaussian': {'factor': 0.3},
        'poisson': {'scaling_factor': 30.0},
    }

    models = {}
    histories = {}
    noisy_images = {}
    denoised_images = {}
    metrics = {}

    for noise_type, params in noise_configs.items():
        print(f"\nTraining Noise2Noise model for {noise_type} noise...")

        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=params)

        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=params)
        noisy_images[noise_type] = X_test_noisy

        model = create_resnet152_with_skips()
        history = model.fit(X_train_noisy1, X_train_noisy2,
                            epochs=epochs,
                            batch_size=32,
                            shuffle=True,
                            validation_split=0.1,
                            verbose=1)

        models[noise_type] = model
        histories[noise_type] = history
        denoised_images[noise_type] = model.predict(X_test_noisy)
        metrics[noise_type] = model.evaluate(X_test_noisy, X_test)

        visualize_results(
            X_test[:3],
            X_test_noisy[:3],
            denoised_images[noise_type][:3],
            noise_type=noise_type
        )

        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    print("\nTest Metrics:")
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")


if __name__ == "__main__":
    train_noise2noise(epochs=50)

import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
import tensorflow_datasets as tfds
from tensorflow.keras.applications import ResNet152
from tensorflow.keras.layers import Conv2D, UpSampling2D, Input, concatenate
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
import warnings


# Suppress warnings
warnings.filterwarnings('ignore')


def add_noise(images, noise_type='bernoulli', params=None):
    if params is None:
        params = {}

    noisy_images = images.copy()

    if noise_type == 'bernoulli':
        prob = params.get('prob', 0.05)
        mask = np.random.binomial(n=1, p=prob, size=images.shape)
        salt = np.random.binomial(n=1, p=0.5, size=images.shape)
        noisy_images = np.where(mask == 1, salt, images)

    return np.clip(noisy_images, 0., 1.)


def preprocess(image, label):
    image = tf.cast(image, tf.float32) / 255.0
    image = tf.image.resize(image, [224, 224])
    return image, image


def prepare_data():
    dataset, info = tfds.load('caltech101', with_info=True, as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

    BATCH_SIZE = 32
    train_dataset = train_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
    test_dataset = test_dataset.map(preprocess).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

    X_train = np.concatenate([x for x, _ in train_dataset], axis=0)
    X_test = np.concatenate([x for x, _ in test_dataset], axis=0)

    return X_train, X_test


def create_resnet152_with_skips(input_shape=(224, 224, 3)):
    base_model = ResNet152(include_top=False, weights='imagenet', input_shape=input_shape)

    # Encoder features
    skip1 = base_model.get_layer("conv1_conv").output        # 112x112
    skip2 = base_model.get_layer("conv2_block3_1_relu").output  # 56x56
    skip3 = base_model.get_layer("conv3_block8_1_relu").output  # 28x28
    skip4 = base_model.get_layer("conv4_block36_1_relu").output # 14x14
    skip5 = base_model.get_layer("conv5_block3_1_relu").output  # 7x7
    base_output = base_model.output                          # 7x7

    # Decoder with feature pyramid structure
    x = Conv2D(2048, (3, 3), activation='relu', padding='same')(base_output)
    x = concatenate([x, skip5])
    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(1024, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip4])
    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(512, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip3])
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(256, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip2])
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(128, (3, 3), activation='relu', padding='same')(x)
    x = concatenate([x, skip1])
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)

    x = UpSampling2D((2, 2))(x)
    x = Conv2D(64, (3, 3), activation='relu', padding='same')(x)
    x = Conv2D(32, (3, 3), activation='relu', padding='same')(x)

    outputs = Conv2D(3, (1, 1), activation='sigmoid', padding='same')(x)

    model = Model(inputs=base_model.input, outputs=outputs)
    model.compile(optimizer=Adam(learning_rate=1e-5), loss='mse', metrics=['mae'])

    return model


def visualize_results(original_images, noisy_images, denoised_images, num_images=3, noise_type=""):
    plt.figure(figsize=(15, num_images * 5))

    for i in range(num_images):
        plt.subplot(num_images, 3, i * 3 + 1)
        plt.imshow(original_images[i])
        plt.title("Original")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 2)
        plt.imshow(noisy_images[i])
        plt.title(f"Noisy ({noise_type})")
        plt.axis("off")

        plt.subplot(num_images, 3, i * 3 + 3)
        plt.imshow(np.clip(denoised_images[i], 0, 1))
        plt.title("Denoised")
        plt.axis("off")

    plt.tight_layout()
    plt.show()


def train_noise2noise_bernoulli(epochs=50, prob=0.05):
    X_train, X_test = prepare_data()

    print("Training data shape:", X_train.shape)
    print("Test data shape:", X_test.shape)

    params = {'prob': prob}
    noise_configs = {
        'bernoulli': {'prob': prob},  # You can add more noise types here
    }
    metrics = {}

    for noise_type, noise_params in noise_configs.items():
        print(f"Training with {noise_type.capitalize()} noise...")

        # Add noise to the dataset
        X_train_noisy1 = add_noise(X_train, noise_type=noise_type, params=noise_params)
        X_train_noisy2 = add_noise(X_train, noise_type=noise_type, params=noise_params)
        X_test_noisy = add_noise(X_test, noise_type=noise_type, params=noise_params)

        # Create and train the model
        model = create_resnet152_with_skips()
        history = model.fit(
            X_train_noisy1, X_train_noisy2,
            epochs=epochs,
            batch_size=32,
            shuffle=True,
            validation_split=0.1,
            verbose=1
        )

        # Evaluate the model
        test_metrics = model.evaluate(X_test_noisy, X_test, verbose=0)
        metrics[noise_type] = test_metrics  # Save metrics: [loss, mae]

        # Predict and visualize results
        denoised_images = model.predict(X_test_noisy)
        visualize_results(X_test[:3], X_test_noisy[:3], denoised_images[:3], noise_type=noise_type.capitalize())

        # Plot training history
        plt.figure(figsize=(12, 4))
        plt.subplot(1, 2, 1)
        plt.plot(history.history['loss'])
        plt.plot(history.history['val_loss'])
        plt.title(f'Noise2Noise Model Loss ({noise_type.capitalize()})')
        plt.ylabel('Loss')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])

        plt.subplot(1, 2, 2)
        plt.plot(history.history['mae'])
        plt.plot(history.history['val_mae'])
        plt.title(f'Noise2Noise Model MAE ({noise_type.capitalize()})')
        plt.ylabel('MAE')
        plt.xlabel('Epoch')
        plt.legend(['Train', 'Validation'])
        plt.show()

    # Print metrics for all noise types
    for noise_type in noise_configs.keys():
        print(f"\n{noise_type.capitalize()} Noise:")
        print(f"Loss: {metrics[noise_type][0]:.4f}")
        print(f"MAE: {metrics[noise_type][1]:.4f}")



if __name__ == "__main__":
    train_noise2noise_bernoulli(epochs=50, prob=0.05)

